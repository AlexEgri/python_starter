# -*- coding: utf-8 -*-
# !/usr/bin/env python
import regex as re
import json
import helpers

from bs4 import BeautifulSoup

_URL = (
    'https://gov.mt/en/Government/Government%20of%20Malta/Administration/Pages/'
    'The-Administration.aspx'
)

_HONORIFICS_PATTERNS = [
    '(^|\W)+{}\W+'.format(honorific) for honorific in
    ['(?:The )?Hon', 'Dr', 'Mr', 'Ms', 'Prof']
]

_SUFFIXES_PATTERN = '|'.join([
    '[, ]\s*{}(\W|$)'.format(suffix) for suffix in
    ['B(?:E&|\.)?A', 'BPlan', 'M[DP]', 'LL[BD]', 'BSc', 'B\.?Com', 'KUOM']
])


class Entity:
    """
    Class representing the partial entity generated by this scraper.
    """

    def __init__(self, lines):
        """
        Build entity from raw values.

        :param lines: list containing the lines of text corresponding to the
            entity.
        """

        # Get the name
        name = lines.pop().strip()
        for pattern in _HONORIFICS_PATTERNS:
            name = re.sub(pattern, '', name)
        name = re.split(_SUFFIXES_PATTERN, name, 1)[0]
        self.name = name.strip()

        # Generate the id based on the name
        self.id = ''.join(
            s.title() for s in re.findall(re.compile('[A-Za-z]'), self.name))

        # Get the political position
        self.position = lines.pop(0).strip()

        # Get the institution
        self.institution = lines[0].strip() if lines else None

    def json(self):
        """
        Return entity in standard JSON format.
        """

        dictionary = {
            '_meta': {
                'id': self.id,
            },
            'name': self.name,
            'fields': [
                d for d in
                [
                    {'tag': 'url', 'value': _URL},
                    {'tag': 'political_position', 'value': self.position},
                    {'name': 'Institution', 'value': self.institution}
                ] if d['value']
            ]
        }

        return json.dumps(
            dictionary, ensure_ascii=False, sort_keys=True, indent=2)


def full_text(elem):
    """
    Return the full text content of the element preserving line breaks.

    :param elem: a BeautifulSoup object.
    """

    previous_elem_ids = []

    def recursive_full_text(elem, text):

        previous_elem = elem.find_previous_sibling()

        should_add_break = (
            previous_elem and id(previous_elem) not in previous_elem_ids and
            previous_elem.name in ['div', 'p', 'br']
        )

        if should_add_break:
            previous_elem_ids.append(id(previous_elem))
            text += '\n'

        if isinstance(elem, str):
            elem = elem.replace(
                '\xa0', ' ').replace('\u200b', '').replace('eâ€™', 'Ã©')
            if '\n' in elem:
                if elem == '\n':
                    return text
                return text + elem.replace('\n', ' ')
            return text + elem

        for child in elem.childGenerator():
            text = recursive_full_text(child, text)

        return text

    full_text = recursive_full_text(elem, '')
    full_text = re.sub('\n\n\n\n+', '\n\n\n', full_text)
    return full_text.strip()


def entities():
    """
    Return generator that yields Entity instances.
    """

    # Get the page
    response = helpers.get(_URL)
    soup = BeautifulSoup(response.content, 'lxml')

    main_div = soup.select_one('div.top-zone')

    relevant_text = full_text(main_div).split(':', 1)[1].strip()

    for snippet in re.split('\s*\n\s*\n\s*', relevant_text):
        # Snippets are blocks of text separated by at least 3 line breaks.
        # Each line in a snippet correspond to a different piece of information.
        lines = snippet.split('\n')
        if len(lines) <= 3:
            # Usually each block of text is about a single entity
            yield Entity(lines)
        elif len(lines) == 4:
            # However, the information for multiple entities are glued together.
            # We can know this based on the number of lines in a snippet. If
            # there are 4 lines, there must be at least 2 entities because
            # there's never more than 3 lines of information per entity.
            # If there are 4 lines, we know the first 2 correspond to the first
            # entity and the last 2 correspond to the second.
            yield Entity(lines[:2])  # First entity
            yield Entity(lines[2:])  # Second entity
        elif len(lines) == 6:
            # If there are 6 lines, we know the first 3 correspond to the first
            # entity and the last 3 correspond to the second.
            yield Entity(lines[:3])  # First entity
            yield Entity(lines[3:])  # Second entity
        # If there are 5 or more than 6 lines glued together, then it's hard to
        # tell what corresponds to what. Luckily as of the writting of this
        # scraper this case wasn't present in the page. If the website changes
        # for the next administration this is a potential place where the
        # scraper could break.


def main():
    """
    Main function of the scraper. Handles outputting the data
    """

    for entity in entities():
        print(':ACCEPT:{}'.format(entity.json()))


if __name__ == "__main__":
    main()